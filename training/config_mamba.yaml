model_name: "state-spaces/mamba-2.8b-hf" # Or other Mamba variants
output_dir: "outputs/mamba-finetune"
max_seq_length: 8192 # Mamba handles long context well
load_in_4bit: true
learning_rate: 2e-4
batch_size: 4
gradient_accumulation_steps: 4
max_steps: 1000
warmup_steps: 100
logging_steps: 10
save_steps: 200
model_type: "mamba" # Hint for training scripts to handle specific loading if needed

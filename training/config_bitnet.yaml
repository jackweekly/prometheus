model_name: "meta-llama/Meta-Llama-3-8B-Instruct"
output_dir: "outputs/bitnet-llama"
max_seq_length: 2048
load_in_4bit: true # Use 4-bit quantization to fit in VRAM
use_bitnet: true # Custom flag to trigger BitNet specific logic
learning_rate: 1e-4
batch_size: 1 # Reduced to prevent OOM
gradient_accumulation_steps: 16 # Increased to maintain effective batch size
max_steps: 1000
warmup_steps: 100
logging_steps: 10
save_steps: 200
dataloader_num_workers: 16 # Utilize 193GB RAM for fast data pre-processing

model_name: "Qwen/Qwen2.5-1.5B-Instruct" # Public default; override with MODEL_ID env
model_type: "jamba"
output_dir: "outputs/jamba-mini-grpo"
max_seq_length: 16384
learning_rate: 1e-4
batch_size: 1
gradient_accumulation_steps: 8
max_steps: 800
warmup_steps: 80
logging_steps: 10
save_steps: 200
lora_r: 16
lora_alpha: 32
lora_dropout: 0.05
# Covers both attention and SSM projections; adjust if the model exposes different module names
target_modules: ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj", "in_proj", "x_proj", "dt_proj"]
group_size: 4  # Number of generations per prompt for GRPO
# Ternary modes: "none" (default), "attention" (ternary attention/MLP), "full" (attention + SSM projections)
ternary_mode: "none"
# Optional: URL returning JSON list of tasks [{prompt, answer, tests?}]
task_url: ""
# Optional: research RSS/Atom feeds (e.g., arXiv)
research_sources:
  - "https://export.arxiv.org/rss/cs.CL"
# Keywords to reward in summaries (case-insensitive)
research_keywords: ["transformer", "mamba", "bitnet", "grpo", "self-improvement"]
# Enable/disable interactive human hints
human_feedback: false
# Max iterations for the online loop; null for unbounded
max_iters: null
# Log a few samples (prompt/completion/reward) per iteration
log_samples: true
log_samples_limit: 2
# Log reward reasons for transparency
log_reasons: true
# Max new tokens per completion (shorter encourages direct answers)
max_new_tokens: 64
# Self-train steps per iteration (SFT on high-reward samples)
self_train_steps: 1
self_train_batch_size: 1
self_train_lr: 2e-6

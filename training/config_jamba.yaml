model_name: "Qwen/Qwen2.5-1.5B-Instruct" # Public default; override with MODEL_ID env
model_type: "jamba"
output_dir: "outputs/jamba-mini-grpo"
max_seq_length: 16384
learning_rate: 1e-4
batch_size: 1
gradient_accumulation_steps: 8
max_steps: 800
warmup_steps: 80
logging_steps: 10
save_steps: 200
lora_r: 16
lora_alpha: 32
lora_dropout: 0.05
# Covers both attention and SSM projections; adjust if the model exposes different module names
target_modules: ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj", "in_proj", "x_proj", "dt_proj"]
group_size: 4  # Number of generations per prompt for GRPO
# Ternary modes: "none" (default), "attention" (ternary attention/MLP), "full" (attention + SSM projections)
ternary_mode: "none"
# Optional: URL returning JSON list of tasks [{prompt, answer, tests?}]
task_url: ""
# Optional: research RSS/Atom feeds (e.g., arXiv)
research_sources:
  - "https://export.arxiv.org/rss/cs.CL"
# Keywords to reward in summaries (case-insensitive)
research_keywords: ["transformer", "mamba", "bitnet", "grpo", "self-improvement"]
